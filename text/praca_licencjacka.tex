\documentclass[declaration,shortabstract,masc]{iithesis}

\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{xpatch}
\newcommand{\cpp}{C\raisebox{0.5ex}{{\tiny\textbf{++}}}}
\newtheorem{Def}{Definicja}
\newtheorem{Fa}{Fakt}
\newtheorem{Lem}{Lemat}
\newtheorem{Tw}{Twierdzenie}
% b - begin
% dwuliterowa nazwa
% n - name
% i - inline
% e - end
\newcommand{\bdf}{\begin{Def}~\\\normalfont\indent}
\newcommand{\bdfn}[1]{\begin{Def}{#1}~\\\normalfont\indent}
\newcommand{\bdfi}{\begin{Def}\normalfont\indent}
\newcommand{\bdfni}[1]{\begin{Def}{#1}\normalfont\indent}
\newcommand{\edf}{\end{Def}\medskip}
\newcommand{\bfa}{\begin{Fa}~\\\normalfont\indent}
\newcommand{\bfan}[1]{\begin{Fa}{#1}~\\\normalfont\indent}
\newcommand{\bfai}{\begin{Fa}\normalfont\indent}
\newcommand{\bfani}[1]{\begin{Fa}{#1}\normalfont\indent}
\newcommand{\efa}{\end{Fa}\medskip}
\newcommand{\ble}{\begin{Lem}~\\\normalfont\indent}
\newcommand{\blen}[1]{\begin{Lem}{#1}~\\\normalfont\indent}
\newcommand{\blei}{\begin{Lem}\normalfont\indent}
\newcommand{\bleni}[1]{\begin{Lem}{#1}\normalfont\indent}
\newcommand{\ele}{\end{Lem}\medskip}
\newcommand{\btw}{\begin{Tw}~\\\normalfont\indent}
\newcommand{\btwn}[1]{\begin{Tw}{#1}~\\\normalfont\indent}
\newcommand{\btwi}{\begin{Tw}\normalfont\indent}
\newcommand{\btwni}[1]{\begin{Tw}{#1}\normalfont\indent}
\newcommand{\etw}{\end{Tw}\medskip}
\xpatchcmd{\qed}{\leavevmode}{\par\nobreak\leavevmode}{}{}
\declaretheorem{theorem} 
\declaretheoremstyle[spaceabove=-6pt, spacebelow=6pt, headfont=\normalfont\itshape, postheadspace=1em, qed=\qedsymbol, headpunct={}]{mystyle}
\declaretheorem[name={Dowód},style=mystyle,unnumbered,]{Proof}
\newcommand{\bdd}{\begin{Proof}~\\\normalfont\indent}
\newcommand{\edd}{\end{Proof}}
\declaretheorem[name={Szkic dowodu},style=mystyle,unnumbered,]{ProofSketch}
\newcommand{\bsd}{\begin{ProofSketch}~\\\normalfont\indent}
\newcommand{\esd}{\end{ProofSketch}}

\polishtitle{Analiza teoretyczna i porównawcza\fmlinebreak efektywnej pamięciowo, wysoce wydajnej\fmlinebreak tablicy asocjacyjnej SILT\fmlinebreak (Small Index Large Table)}
\englishtitle{Theoretical and Comparative Analysis of\fmlinebreak Memory-Efficient, High-Performance\fmlinebreak Associative Array SILT\fmlinebreak (Small Index Large Table)}
\polishabstract{SILT (Small Index Large Table) jest efektywną pamięciowo, wysoce wydajną tablicą asocjacyjną. Stanowi alternatywę dla innych popularnych struktur danych opracowanych na potrzeby rozwiązania problemu przechowywania par klucz-wartość. Została utworzona z myślą o ograniczeniu i zbalansowaniu zużycia pamięci oraz czasu obliczeń. W swojej pracy chcę się skupić na analizie teoretycznej tej struktury danych oraz na porównaniu jej działania z dostępną w bibliotece standardowej języka \cpp\ funkcją \texttt{std::unordered\_map}. W ramach analizy opiszę konstrukcję tej struktury obejmującą trzy główne części składowe -- SortedStore, HashStore oraz LogStore, jak również metodę haszowania kukułczego, która jest w niej wykorzystywana. Zaimplementuję tę strukturę danych i zbadam empirycznie jej wydajność. Porównam ją z wynikami teoretycznymi i doświadczalnymi, przedstawionymi przez jej twórców.}
\englishabstract{\indent SILT (Small Index Large Table) is memory-effective high-performance associative array. It is an alternative for other popular data structures that solve key-value storage problem. It was developed for purpose of reducing and balancing memory usage and computation time. In my thesis I would like to focus on theoretical analysis of this data structure and comparison with \cpp\ standard library's function \texttt{std::unordered\_map}. As part of analysis I will describe its construction consisting of three main parts -- SortedStore, HashStore, LogStore and cuckoo hashing method. I will implement this structure and empirically examine its performance. I~will compare it with theoretical and experimental results, shown by its authors.}
\author{Paweł Guzewicz}
\advisor{dr hab. Marek Piotrów}
\transcriptnum{263664}
\advisorgen{dr. hab. Marka Piotrowa}
%\usepackage{graphicx,listings,amsmath,amssymb,amsthm,amsfonts,tikz}
%\theoremstyle{definition} \newtheorem{definition}{Definition}[chapter]
%\theoremstyle{remark} \newtheorem{remark}[definition]{Observation}
%\theoremstyle{plain} \newtheorem{theorem}[definition]{Theorem}
%\theoremstyle{plain} \newtheorem{lemma}[definition]{Lemma}
%\renewcommand \qedsymbol {\ensuremath{\square}}

\begin{document}
	\chapter{Wprowadzenie}
		\section{Przeznaczenie i zastosowanie}
			Small Index Large Table (SILT) to złożona struktura danych zaprojektowana z~myślą o efektywnej realizacji operacji słownikowych
			\begin{enumerate}
				\item
					Wstawiania: \texttt{insert(Key key, Value value)}
				\item
					Usuwania: \texttt{remove(Key key)}
				\item
					Pobrania wartości: \texttt{get\_value(Key key)}
			\end{enumerate}
			gdzie \texttt{Key} oraz \texttt{Value} oznaczają odpowiednio typ klucza i typ wartości.\\
			\indent W założeniu ma ona szybko obsługiwać powyższe operacje przy zrównoważonym, niedużym zużyciu pamięci. Struktura ta ma być bowiem wykorzystywana w~przypadku konieczności przechowywania dużej ilości danych. Podobnie jak w strukturach bazo-danowych, także tu, kluczowym czynnikiem jest czas dostępu do pamięci dyskowej. (Przyjmujemy dla celów analizy, że jest to pamięć flash, podobnie jak w oryginalnej pracy \cite{SILT}.) Tablica SILT jest zaprojektowana tak, aby oszczędnie korzystać z tejże pamięci. Znajduje swe zastosowanie np. przy przetwarzaniu dużych danych (Big data) jako wariant bazy typu NoSQL.
		\section{Koncepcje i cechy}
			Oczekujemy realizacji wymienionych operacji słownikowych w asymptotycznym czasie stałym. Oczywiście należy brać pod uwagę możliwość wykorzystania cache'u procesora, który zmniejsza czas dostępu do danych i jednocześnie długie czasy dostępu do pamięci zewnętrznej (pamięć flash lub dysk twardy), których chcemy uniknąć.\\
			\indent Uwzględniając możliwości i ograniczenia sprzętowe należy stworzyć strukturę danych dopasowaną możliwie najlepiej do podanych operacji. Nie jest to łatwe, gdy dysponujemy jednym rodzajem magazynu danych. Rozwiązaniem jest połączenie kilku różnych struktur w~jedną. W każdej z tych podstruktur nacisk kładziony jest na inny aspekt -- możliwość szybiego wykonywania operacji modyfikujących strukturę słownika (\texttt{insert(Key key, Value value)} lub \texttt{remove(Key key)}) bądź ograniczenie pamięci kosztem rezygnacji z tych operacji i ograniczenia interfejsu do pobierania wartości (\texttt{get\_value(Key key)}). W~wyniku otrzymujemy kompromisowe rozwiązanie, które okazuje się być bardzo wydajne.\\
			\indent Innym celem, który chcemy osiągnąć jest skalowalność przy ograniczonym zużyciu pamięci przypadającej na jeden klucz.
		\section{Ogólny schemat działania}
			SILT składa się z trzech podstruktur: LogStore, kilku egzemplarzy HashStore i~SortedStore, które są ze sobą powiązane.\\
			\indent Pierwsza z nich odpowiada za szybką obsługę operacji wstawiania i usuwania. Wykorzystuje ona znaną technikę dopisywania kolejnych elementów (par klucz-wartość) na końcu dziennika (logu) -- pliku w pamięci flash. Jest to wysoce wydajne rozwiązanie, lecz nie zapewnia możliwości wyszukiwania, gdyż klucze są umieszczone w pliku w kolejności wstawiania. Do tego celu LogStore w~pamięci RAM przechowuje tablicę z haszowaniem, opartą o metodę haszowania kukułczego, w której zapamiętane są pozycje (przesunięcie od początku pliku) par klucz-wartość w pliku dziennika.\\
			\indent Podstruktury drugiego rodzaju (HashStore'y) są skompresowaną wersją LogStore'ów. Nowa jednostka powstaje w wyniku konwersji przepełnionego LogStore'u. HashStore'y również zawierają tablice z haszowaniem. Pozbawione są jednak informacji o przesunięciu (klucze są sortowane według haszy), przez co efektywniej wykorzystują pamięć. Ich jedynym zadaniem jest obsługa operacji pobrania wartości, wobec czego mogą być strukturami tylko do odczytu. Podobnie jak w LogStore'rze wszystkie dane trzymane są w pamięci flash, a w pamięci RAM zostaje umieszczony filtr (okrojona tablica z haszowaniem), który wstępnie określa, czy dany klucz może znajdować się w tym magazynie (przy niskim odsetku fałszywych pozytywnych wskazań, z ang. \textit{false positives}).\\
			\indent W trzeciej podstrukturze (SortedStore) w pamięci flash trzymane są posortowane dane. Eliminuje to konieczność używania tablicy z haszowaniem do wyszukiwania pozycji w pliku. Przy takim podejściu potrzebowalibyśmy jednak czasu $O\left(\log n\right)$, używając wyszukiwania binarnego, na znalezienie naszej wartości. Również i tu możemy zastosować efektywniejsze rozwiązanie -- indeksowanie za pomocą drzew trie trzymanych w pamięci RAM.
	\chapter{Struktura danych Small~Index~Large~	Table}
		\label{logstore}
		\section{LogStore}
			Pary klucz-wartość przechowywane w strukturze SILT mają pewnien stały, ustalony rozmiar. Może on jednak być dosyć duży, więc zostanie zapisany w pamięci flash. W pamięci RAM przechowujemy zaś tablicę z haszowaniem kukułczym (a dokładniej w pewnym jego wariancie -- haszowaniu kukułczym dla częściowych kluczy).\\
			\indent W podstawowej wersji haszowania kukułczego używamy tablicy kubełków (ustalonego rozmiaru) oraz dwóch funkcji haszujących $h_1$ i $h_2$. Dla danego klucza wyznaczają one dwie potencjalne lokalizacje (kubełki), w które można wstawić nowy klucz. Jeśli w którymś z kubełków jest wolne miejsce, to wstawiamy do tego niezapełnionego kubełka nasz klucz. W przeciwnym razie wstawiamy nowy klucz do jednego z kubełków mimo zapełnienia i wypychamy znajdujący się w nim klucz (można myśleć, że podrzucamy kukułcze jajo do gniazda). Następnie wypchnięty klucz przenosimy do jego alternatywnej lokalizacji (drugiego kubełka). To również może powodować konieczność wypchnięcia kolejnego klucza. Kontynuujemy proces wstawiania i wypychania aż do znalezienia wolnego kubełka lub do momentu, gdy przekroczymy maksymalną liczbę wypchnięć. Jeśli osiągniemy limit iteracji i nie udało nam się wstawić elementu, to należy ponownie shaszować elementy tablicy używając nowych funkcji haszujących. Może być też konieczne zwiększenie rozmiaru tablicy, jeśli współczynnik zapełnienia osiągnął odpowiednią wartość (używając techniki podwajania wielkości utrzymujemy zamortyzowany czas stały).\\
			\indent W naszym przypadku, z uwagi na potencjalnie duży rozmiar klucza, unikamy wykorzystywania go w całości. SILT używa 160-bitowych kluczy, które pochodzą z rozkładu jednostajnego i są wstępnie obliczane przy użyciu algorytmu SHA-1. Dzięki temu otrzymujemy stałą długość klucza. Aby jeszcze zmniejszyć rozmiar kluczy w tablicy z haszowaniem kukułczym stosujemy tzw. częściowe klucze lub odciski palca (z ang. \textit{fingerprints}).\\
			\indent Haszowanie kukułcze dla częściowych kluczy (z ang. \textit{partial-key cuckoo hashing}) \cite{PKCH} to zmodyfikowana wersja haszowania kukułczego. Zamiast zapamiętywać cały 160-bitowy klucz zapamiętujemy pewnien jego znacznik. To w oczywisty sposób uniemożliwia odtworzenie z niego klucza i tym samym powoduje problem w momencie konieczności obliczenia alternatywnego kubełka. Wystarczy więc wraz z danymi przechowywanymi w komórce tablicy związać informację o drugim kubełku. W LogStore'rze będziemy używali do tego celu $14$-bitowych znaczników, zatem kubełki w tablicy będą indeksowane liczbami od $0$ do $2^{14} - 1$. Łączny rozmiar pojedynczego wpisu w kubełku to 4 bajty. Zawiera on 14 bitów znacznika, jeden bit określający rodzaj operacji ($0$ dla usuwania, $1$ dla wstawiania), jeden bit zajętości ($0$, gdy wpis jest nieużywany (wolny) oraz $1$, gdy zajęty) oraz dwubajtową wartość przesunięcia od początku pliku (z ang. \textit{offset}).\\
			\indent Znaczniki odpowiadają funkcjom $h_1$ oraz $h_2$ zaaplikowanym do klucza. Aby sprawdzić czy dany wpis w tablicy dotyczy klucza, dla którego właśnie szukamy wartości, należy obliczyć dla niego znaczniki i sprawdzić czy w kubełku indeksowanym $h_1$ znajduje się znacznik $h_2$ i odwrotnie. Schematycznie (pomijając informacje zakodowane w dwóch dodatkowych bitach oraz głębokość kubełków) dane są związane relacją \texttt{hash\_table[h1] = (h2, offset)}. Autorzy pracy \cite{SILT} ustalili eksperymentalnie optymalną głębokość kubełków na $4$. Co daje łącznie $2^{16}$ wpisów w tablicy. Znaczniki obliczane są jako rozłączne fragmenty $14$-bitowe wybrane spośród najmłodszych bitów 160-bitowego klucza. Przyjmujemy również ograniczenie na liczbę iteracji wstawień i wypychań na $64$. Jeśli przekroczymy tę liczbę, oznacz to, że tablica jest już prawie pełna.\\
			\indent Warto zaznaczyć, że w oryginalnej wersji znacznik zawiera $15$ bitów. Uniemożliwia to jednak kodowanie operacji. Dla celów analizy wydajności LogStore'u pominięcie operacji usuwania nie ma żadnego znaczenia. Operacje wstawiania i usuwania elementów z LogStore'u z punktu widzenia struktury są niemal identyczne. Różnią się jedynie zapisaniem informacji o rodzaju operacji, wobec czego analiza usuwania sprowadza się do analizy wstawiania. Taki wybór rozmiaru znacznika niesie za sobą jednak dwie konsekwencje. Z jednej strony zyskujemy dwukrotnie większą przestrzeń adresową, przez co mamy do dyspozycji $2^{17}$ wpisów w tablicy. Jednak pociąga to za sobą konieczność rozszerzenia rozmiaru wpisu o $2$ bajty. Zwiększa się wówczas rozmiar offsetu, gdyż zmienna $16$-bitowa byłaby niewystarczająca. Wydaje się, że autorzy pracy postąpili nieco wbrew przyjętej przez nich konwencji. W pozostałych strukturach zawsze starali się zaoszczędzić pamięć, jeśli to tylko było możliwe. Ponadto ograniczyli także odrobinę funkcjonalność struktury danych. Zmianę tę należy potraktować jako kosmetyczną poprawkę.\\
		\section{HashStore}
			HashStore to kompaktowa wersja LogStore. W momencie, gdy osiągniemy zapełnienie LogStore jest konwertowany do HashStore. Konwersja polega na przepisaniu danych z dziennika (logu) do nowego pliku tak, aby były uporządkowane w kolejności haszy, a nie -- jak poprzednio -- w kolejności wstawiania. Dzięki temu możemy zmniejszyć liczbę koniecznych do zapamiętania informacji. Wystarczy we wpisach tablicy pamiętać jedynie znaczniki, pomijając przesunięcia. Można je bowiem wyliczyć, korzystając z ich uporządkowania i ze stałego rozmiaru par klucz-wartość.\\
			\indent Po zgromadzeniu dostatecznie dużej liczby (ustalona z góry stała - w naszym przypadku równa $15$) HashStore'ów należy je posortować i scalić z dotychczasowym SortedStore'em. Proces sortowania przebiega sekwencyjnie. Sortujemy po kolei każdy HashStore z osobna. Do tego celu używamy algorytmu sortowania pozycyjnego (\texttt{radix\_sort} \cite{Cormen}) dla kluczy $160$-bitowych. Składa się on z $10$ przebiegów algorytmu sortowania przez zliczanie (\texttt{counting\_sort}) począwszy od najmniej znaczących par bajtów (sortujemy przez zliczanie $16$-bitowe fragmenty). Liczba kubełków jest stała i nieduża, równa $2^{16} = 65536$.\\
			\indent Kolejnym krokiem, mając posortowane HashStore'y, jest scalanie. Będziemy dokonywać wielotorowego scalania (z ang. \textit{multiway merge}), początkowo wybierając element, który ma się znaleźć w nowym SortedStore'rze na podstawie jego klucza, rodzaju operacji oraz chronologii. Jeśli mamy tylko jeden klucz, to go wybieramy. W przypadku konfliktu (2 lub więcej takich samych) sprawdzamy, który z nich był najpóźniej dodany (czyli posiada najświeższą informację). Następnie, po wyborze klucza, odczytujemy rodzaj operacji (dla kluczy z SortedStore jest to zawsze dodawanie) i dopisujemy go bądź nie do pliku wynikowego. Do wybierania kluczy używamy algorytmu znanego z sortowania przez scalanie (tam scalając dwie listy przesuwamy się wskaźnikiem na kolejne elementy, tu mamy do czynienia z większą liczbą list i wskaźników). Problematyczne jest wybranie kandydata spośród $15$ lub $16$ kluczy (w zależności, czy był już wcześniej jakiś SortedStore). Aby przyspieszyć i ułatwić ten proces używamy kopca binarnego do przechowywania kluczy na pozycjach wskazywanych przez wskaźnik. Podczas scalania w kopcu znajduje się co najwyżej $15$ (lub $16$) kluczy. Wybierając element ze szczytu kopca (uporządkowanego według naszego kryterium) decydujemy czy dopisać go do pliku czy nie, a następnie usuwamy z kopca wszystkie elementy o tym samym kluczu oraz przywracamy własność kopca, a dla elementów, które usunęliśmy przesuwamy wskaźnik na ich liście na kolejny klucz i wstawiamy go do kopca. Iterujemy tę operację, aż kopiec będzie pusty.
		\section{SortedStore}
			Po scaleniu HashStore'ów budujemy indeksowanie dla posortowanego pliku złożonego z par klucz-wartość. Będzie ono oparte na drzewach trie (czyt. \textit{traj}). Są to drzewa binarne o krawędziach etykietowanych zerami i jedynkami (można pominąć etykiety w pamięci -- $0$ odpowiada zawsze lewemu poddrzewu, a $1$ prawemu). Dla danej tablicy liczb w zapisie binarnym każdy liść w takim drzewie odpowiada dokładnie jednej z nich. Numer liścia (liczony od lewej) wyznacza indeks w tablicy. W naszym przypadku ten indeks to przesunięcie pary klucz-wartość w pliku wynikowym. Każdy wierzchołek wewnętrzny odpowiada najdłuższemu wspólnemu zero-jedynkowemu prefiksowi. Ponadto w drzewie tym trzymane jest jedynie tak wiele wierzchołków na ścieżce od korzenia do liścia, żeby etykiety krawędzi tworzyły najkrótszy jednoznaczny prefiks (a ang. \textit{shortest unique prefix}). Aby zapamiętać drzewo trie, zamiast kosztownych pamięciowo wskaźników, zapamiętujemy w każdym wierzchołku liczbę wierzchołków w jego lewym poddrzewie. W ten sposób jesteśmy w stanie łatwo obliczyć numer każdego liścia. Taka reprezentacja jest też łatwa w kodowaniu. Wystarczy zapisać ją zgodnie z rekurencyjną zależnością
			$$representation\left(T\right) = |L| \cdot representation\left(L\right) \cdot representation\left(R\right),$$
			gdzie $T$ to całe drzewo, $L$ - lewe poddrzwo, $R$ - prawe poddrzewo, $|L|$ to liczba wierzchołków w lewym poddrzewie, a $\cdot$ to operacja sklejenia.\\
			\indent Aby odnaleźć liść w drzewie posługujemy się rekurencyjnym algorytmem, który odwiedza odpowiednie poddrzewo i zapamiętuje sumę etykiet wierzchołków, w których wybrał przejście do prawego poddrzewa (tym samym liczbę liści, które zostały na lewo od poszukiwanego wierzchołka).\\
			\indent Aby ograniczyć potencjalnie kosztowny proces rekurencyjnego przeszukiwania można podzielić drzewa trie do $2^k$ kubełków na podstawie $k$ pierwszych bitów klucza. Pozwala to na dobranie odpowiedniego $k$ np. równego 10, aby zredukować oczekiwaną liczbę kluczy, które znajdą się w poszczególnych drzewach trie. Autorzy podają, że w przypadku danych rozmiaru $2^{16}$ oczekujemy $2^{16-10} = 2^6$ kluczy w drzewie, a z dużym prawdopodobieństwem nie więcej niż $2^8$, czyli stałą liczbę.\\
			\indent Autorzy pracy o SILT przedstawili jeszcze jedną, skomplikowaną reprezentację drzew trie, która jest w stanie jeszcze efektywniej zapamiętywać informacje o kluczach. Średnio wystarcza w niej około $0,4$ bajta na sekundkę.
		\section{Przepływ danych wewnątrz struktury}
			Jak widać z opisanej konstrukcji przepływ danych w strukturze jest spowolniony, przypomina przepychanie informacji (od LogStore'u do SortedStore'u poprzez HashStore'y). Leniwe wstawianie i usuwanie oraz konwersje ze struktur jednego typu do innego występujące co jakiś czas mają na celu zamortyzowanie czasu operacji.
		\section{Rozszerzenia}
			Struktura SILT może być rozszerzona tak, aby umożliwić równoległe przetwarzanie transakcji (w rozumieniu bazy danych). Dodatkowo można jeszcze bardziej amortyzować czas, który jest poświęcany na konwersję z jednego magazynu danych do drugiego stosując konwersję równolegle do obsługi zapytań. Taka wersja została zaimplementowana i opisana przez twórców SILT. Jedną z najistotniejszych różnic jest sposób scalania HashStore'ów, który jest realizowany w tle. Jest on jednak rozciągnięty w czasie, a kolejne struktury scalane są jedna po drugiej, więc struktura kopca, używana do scalania, nie jest potrzebna.\\
			\indent Innym ważnym aspektem jest możliwość odtwarzania danych po awarii. Do tego celu wystarczy zapamiętywać w osobnym dzienniku w pamięci flash kolejne operacje zlecone do LogStore'u. Po awarii, aby odzyskać tablicę trzymaną w pamięci RAM, należy po kolei odtworzyć operacje zapisane w dzienniku. Dla pozostałych magazynów możemy pamiętać kopię informacji z pamięci RAM na dysku, gdyż są one strukturami tylko do odczytu.\\
			\indent W przypadku dużego obciążenia pamięci RAM można, kosztem czasu dostępu, usunąć z niej tablice indeksujące dla HashStore'ów i SortedStore'ów, pod warunkiem, że zostaną zapamiętane w pamięci flash. Dla SortedStore możliwe jest nawet kompletne pominięcie struktury drzew trie, ale wiąże się to z kosztownym przeszukiwaniem binarnym.\\
			\indent Jeśli chcielibyśmy wykorzystać strukturę SILT do przechowywania par klucz-wartość o zmiennej długości, to możemy użyć techniki przechowywania par (przesunięcie, fragment pary klucz-wartość), gdzie przesunięcie odnosi się do odrębnego pliku, w którym zapamiętane są właściwe dane.\\
			\indent W obrębie jednej bazy danych możemy korzystać z kilku magazynów SILT. Każdy odpowiedzialny jest wówczas za osobny (rozłączny z innymi) podzbiór kluczy.
	\chapter{Analiza teoretyczna}
		\section{Haszowanie kukułcze}
			\subsection{Wprowadzenie do analizy}
				Haszowanie kukułcze szeroko opisane w pracy \cite{CH} ma oszacowanie oczekiwanego czasu operacji na poziomie $O(1)$. Wynika to z analizy prawdopodobieństwa wystąpienia łańcucha kolizji podczas wstawiania. Analizowany jest graf dwudzielny wyznaczony przez kolejne kubełki. Kluczowe w analizie jest ograniczenie możliwej liczby iteracji oraz wyznaczenie optymalnej stałej. Po osiągnięciu takiej liczby operacji następuje konieczność przehaszowania tablicy.\\
				\indent Różnicą w analizie, w kontekście haszowania za pomocą częściowych kluczy, jest inny sposób zapewnienia stałego czasu wstawiania. Oczywiście limit iteracji jest zachowany, natomiast tablica nie jest już ponownie haszowana. Decydujemy się wtedy na wycofanie transakcji (wstawiania lub usuwania). Takie odtwarzanie trwa nie więcej niż trwało jego nieudane wstawianie do tablicy z haszowaniem. Wobec czego ta operacja ma co najwyżej czas dwukrotnie dłuższy, czyli dalej stały.\\
				\indent Podobna uwaga dotyczy obliczania wartości funkcji haszujących. W zwykłej wersji obliczaliśmy ją gdy była potrzebna (dla danego klucza mogła być więc obliczana wielokrotnie), a w zmodyfikowanej wersji wszystkie alternatywne kubełki są od razu obliczone. Należy podkreślić, że obliczenie to realizowane jest kosztem dwóch instrukcji maszynowych (wyłuskujemy odpowiedni fragment klucza). Ten klucz został wcześniej shaszowany funkcją SHA-1, ale to było zrobione tylko raz, więc jest to czas stały. W pewnym sensie odrobinę zyskujemy czasowo na obliczaniu funkcji haszujących, tracimy zaś na pamięci.\\
				\indent Poprzednia uwaga nie jest kluczowa w kontekście złożoności. Na potrzeby uzasadnienia poprawności oszacowań dla czasu działania haszowania wystarczy zauważyć, że założenia dotyczące oryginalnego modelu nie zostały osłabione. Funkcje haszujące są dobrze dobrane i niezależne (fragmenty klucza, z których wybieramy znaczniki nie pokrywają się), co wynika z własności kluczy generowanych algorytmem SHA-1.\\
				\indent Ostatnią rzeczą, która wymaga komentarza jest użycie kubełków o rozmiarze większym niż $1$. Wpływa to pozytywnie na możliwość wypełnienia tablicy. Dzięki temu w \cite{SILT} osiągnięto zajętość na poziomie $93$\%. Inny podobny pomysł, tym razem bazujący na dodaniu małego schowka, który ma poprawić zajętość tablicy jest opisany w pracy \cite{CHwS}. Oczywiście aby zachować własności probabilistyczne podczas wypychania kluczy z alternatywnego kubełka losujemy, który wpis ma być usunięty (w naszym przypadku losujemy liczbę od $0$ do $3$). Możemy to zrobić, gdyż kolejność wpisów wewnątrz kubełka nie ma znaczenia.
			\subsection{Właściwa analiza}
				\bdf
					Rodzinę $\left\{h_i\right\}_i\in I$, $h_i: U \to T$ nazywamy $\left(c, m\right)$-uniwersalną, gdy dla dowolnych $m$ różnych elementów $x_1,\ldots,x_m \in U$, dowolnych $y_i,\ldots,y_m \in T$ i indeksu $i \in I$ wybranego losowo z rozkładem jednostajnym
					$$\Pr[h_i(x_1) = y_1,\ldots,h_i(x_m) = y_m] \leq \frac{c}{|T|^m}$$
				\edf
				Przyjrzymy się teraz kanonicznej wersji haszowania kukułczego. Ta wersja różni się nieznacznie od przedstawionej wcześniej w konteście LogStore'a (\ref{logstore}). Zamiast jednej tablicy $T$ o rozmiarze $2 \cdot t$ mamy dwie -- $T_1$ i $T_2$, obie o rozmiarze $t$. Każda z nich jest przypisana do konkretnej funkcji haszującej odpowiednio $h_1$ i $h_2$. Funkcje $h_1, h_2 : U\to\left\{0,\ldots, t - 1\right\}$ oraz pochodzą z $(1, MaxLoop)$-uniwersalnej rodziny funkcji haszujących, gdzie $MaxLoop$ oznacza stałą ograniczającą liczbę iteracji. Jedna komórka tablicy odpowiada jednemu kubełkowi. Dla uproszczenia przyjmujemy, że ma on rozmiar $1$. Obie wersje są sobie równoważne, ale z punktu widzenia analizy złożoności wersja z dwiema tablicami będzie wygodniejsza. Dalsza część analizy bazuje na pracy \cite{CH} i wykładzie \cite{UW}.\\
				\indent Niech $n$ oznacza liczbę kluczy, które chcemy umieścić w słowniku, czyli naszej strukturze złożonej z dwóch tablic $T_1$ i $T_2$, używając haszowania kukułczego oraz niech $t\geq 2\cdot n$. Będziemy analizować pojedyńczą operację wstawienia klucza.\\
				\indent W danej chwili w słowniku znajduje się $m$ kluczy. Rozważmy dwudzielny graf nieskierowany $G=(V,E)$, którego wierzchołki odpowiadają komórkom tablic $T_1$ i $T_2$. Krawędzie w grafie łączą wierzchołki $h_1(k_i)$ z $h_2(k_i)$ dla $i \in \left\{1,\ldots,m\right\}$ i są unikalnie etykietowane kluczami $k_i$.\\
				\indent Wstawiamy $(m+1)$-szy klucz ($k_{m+1}$). Trafia on do jednego ze swoich kubełków, bez straty ogólności do $T_1[h_1(k_{m+1})]$. Niech $x_1,\ldots,x_s$ będzie ciągiem krawędzi odwiedzanych przez operację wstawiania klucza $k = k_{m+1}$ do słownika. Zauważmy, że wyznacza on w grafie $G$ pewną ścieżkę $S$ o długości (liczbie krawędzi) równej $s$, niekoniecznie prostą (dopuszczamy powtarzające się wierzchołki). Przejście krawędzi $e$ odpowiada procedurze wstawienia klucza, który nie ma swojego kubełka (wyjściowo jest to klucz będący argumentem funkcji \texttt{insert}) na siłę do zajętego kubełka i wypchnięciu klucza, który go zajmował do jego alternatywnego kubełka.
				\bfa
					Jeśli $k$ jest kluczem wstawianym do tablicy z haszowaniem kukułczym, to wstawienie powiedzie się, gdy występuje co najwyżej jeden cykl w $S$.
				\efa
				\bsd
					Jeśli w wyniku wypychnięcia kolejnych kluczy odwiedzamy różne kubełki i na końcu trafiamy na pusty, to udało nam się wstawić. Jeśli wróciliśmy do jakiegoś kubełka (mamy cykl), to tym razem klucz $k$ będzie wypychany z powrotem, aż wróci do początkowego kubełka. Wówczas zostanie wstawiony do alternatywnego kubełka, a skoro nie ma więcej cykli, to kolejne wypchnięcia doprowadzą do pomyślnego wstawienia klucza do słownika.
				\esd
				\bfa
					Jeśli $k$ jest kluczem wstawianym do tablicy z haszowaniem kukułczym, to wstawienie nie powiedzie się i konieczne będzie ponowne przehaszowanie tablicy, gdy występują $2$ lub więcej cykle w $S$.
				\efa
				\bsd
					Niech $S = \left(x_1,\ldots,x_i,\ldots,x_l,\ldots,x_s\right)$, gdzie $x_i$ i $x_l$ oznaczają krawędzie zamykające dwa pierwsze cykle, czyli powodujące przemieszczenie kluczy do poprzednio odwiedzonego kubełka. Oczywiście pomiędzy $x_i$ a $x_l$ znajduje się krawędź $x_j$ taka, że klucz wstawiony na początku iteracji ($k$) zostaje przemieszczony do swojego alternatywnego kubełka. Rozważmy prefiks $P$ ścieżki $S$ zawierający dwa początkowe cykle. Zauważmy, że w momencie przejścia ostatniej krawędzi w $P$ klucz wstawiany do słownika ($k$) znów zostanie wypchnięty do swojego pierwszego kubełka. Oznacza to, że kolejne operacje będą powtarzane cyklicznie (zapętlą się, aż osiągną $MaxLoop$ wypchnięć) i nigdy nie znajdziemy miejsca w naszej strukturze.
				\esd
				\ble
					Załóżmy, że $S$ zawiera co najwyżej jeden cykl. Wówczas dla każdego prefiksu $P = \left(x_1,\ldots,x_p\right)$ ścieżki $S$ istnieje podciąg ścieżki $P$ zawierający $\frac{p}{3}$ różnych krawędzi (w sensie etykiet), zaczynający się od $k$, gdzie $k$ jest wstawianym do słownika kluczem.
				\ele
				\bdd
					Jeśli procedura wstawiania nie wraca nigdy do poprzednio odwiedzonego wierzchołka, to $S$ nie zawiera cyklu, więc prefiks jest ciągiem $p$ różnych kluczy (w sensie etykiet krawędzi), zaczynającym się w $k$.\\
					\indent W przeciwnym przypadku $S$ zawiera dokładnie jeden cykl. Wówczas\linebreak
					$S = \left(x_1,\ldots,x_i,\ldots,x_j,\ldots,x_s\right)$, gdzie $x_i$ jest krawędzią zamykającą cykl, a $x_j$ krawędzią przenoszącą wstawiany klucz $k$ do alternatywnego kubełka. Jeśli $p < j$, to początkowe $i \geq \frac{j}{2} \geq \frac{p}{2}$ kluczy tworzy szukany podciąg. Dla $p \geq j$ jeden z ciągów $\left(x_1,\ldots,x_{i-1}\right)$ lub $\left(x_j,\ldots,x_p\right)$ ma długość przynajmniej $\frac{p}{3}$.
				\edd
				Chcemy określić złożoność procedury wstawiania. Rozważmy pojedyńcze wstawienie klucza $k$ do słownika. Chcemy obliczyć jakie jest prawdopodobieństwo tego, że procedura wstawiania trwa przynajmniej $q$ tur (tzn. $q$ razy wypychano klucz). Oczywiście $q \leq MaxLoop$. W przeciwnym przypadku prawdopodobieństwo to wynosi $0$.\\
				\ble
					Prawdopodobieństwo wystąpienia co najwyżej jednego cyklu na ścieżce $S$ o długości $q$ jest ograniczone przez $2 \cdot \left(\frac{1}{2}\right)^{\frac{q}{3} - 1}$.
				\ele
				\bdd
					Na mocy lematu jeśli ścieżka $S$ zawiera co najwyżej jeden cykl, to istnieje podciąg długości $\frac{q}{3}$, zaczynający się od $k$. Zatem prawdopodobieństwo wystąpienia takiej ścieżki $S$ jest ograniczone z góry przez prawdopodobieństwo wystąpienia takiego podciągu (może się on zaczynać od kubełka w tablicy $T_1$ bądź kubełka w tablicy $T_2$), które wynosi
					$$2\cdot n^{\frac{q}{3}-1}\cdot \left(\frac{1}{t}\right)^{\frac{q}{3}-1} = 2 \cdot \left(\frac{n}{t}\right)^{\frac{q}{3} - 1}\leq 2 \cdot \left(\frac{1}{2}\right)^{\frac{q}{3} - 1}$$
				\edd
				\ble
					Prawdopodobieństwo wystąpienia dwóch cykli na ścieżce $S$ jest ograniczone przez $O\left(\frac{1}{n^2}\right)$.
				\ele
				\bsd
					Rozważmy ścieżkę długości $s$. Chcemy oszacować liczbę możliwych wystąpień dwóch cykli.\\
					\indent Niech $k$ będzie ustalonym wierzchołkiem startowym. Szacujemy z góry przez $n^{s - 1}$ liczbę sposobów wyboru pozostałych wierzchołków na ścieżce, przez $s^3$ liczbę możliwych wyborów indeksów dla krawędzi rozpoczynającej cykl, krawędzi zamykającej cykl i krawędzi końcej ($x_s$) oraz na $t^{s - 1}$ sposobów wybieramy wierzchołki grafu, przez które będzie przechodził cykl. Dla jednego z nich nie ma miejsca, więc musimy odjąć jeden od $s$. Otrzymujemy oszacowanie na liczbę cykli równe $n^{s - 1}\cdot s^3\cdot t^{s - 1}$.\\
					\indent Zauważmy jednak, że każda krawędź może znaleźć się na cyklu z prawdopodobieństwem $\left(\frac{1}{t}\right)^2$, gdyż funkcje haszujące pochodzą z rodziny $(1, MaxLoop)$-uniwersalnej i $s \leq MaxLoop$. Co daje nam oszacowanie $\frac{n^{s - 1}\cdot s^3\cdot t^{s - 1}}{t^{2s}}$. Aby otrzymać ostateczne oszacowanie obliczamy sumę po wszystkich możliwych wartościach $s$
					$$
						\displaystyle\sum_{s\geq 3} \frac{n^{s - 1}\cdot s^3\cdot t^{s - 1}}{t^{sr}} \leq O\left(\frac{1}{n^2}\right)
					$$
				\esd
				\btw
					Oczekiwany koszt operacji wstawiania jest stały.
				\etw
				\bsd
					Czas wstawiania możemy obliczyć rozbijając przestrzeń zdarzeń. Obliczamy czas potrzebny w sytuacji, w której nie ma potrzeby przehaszowania tablicy. Wynosi on $O(1) \cdot \left(1 - O\left(\frac{1}{n^2}\right)\right)$. Oszacowanie na koszt pojedyńczej operacji wynika z obliczenia sumy $\displaystyle\sum_s 2 \cdot \left(\frac{n}{t}\right)^{\frac{s}{3} - 1}$, którą można oszacować przez $O\left(\displaystyle\sum_r \left(\frac{1}{2}\right)^{\frac{s}{3} - 2}\right) = O(1)$. Czas w przypadku konieczności ponownego haszowania wynosi $O(n\log n)\cdot O\left(\frac{1}{n^2}\right)$. Oszacowanie na koszt pojedyńczej operacji wynika z oszacowania na wartość $MaxLoop$, które jest równe $O(\log n)$.\\
					\indent Po dodaniu do siebie rozłącznych możliwosci otrzymujemy $O(1)$.
				\esd
		\section{Fałszywe pozytywne wskazania}
			Dla każdego klucza mamy do dyspozycji 2 kubełki po 4 wpisy, w sumie $8=2^3$. Zatem prawdopodobieństwo fałszywie pozytywnego wskazania wynosi
			$$\frac{2^3}{2^{14}} = 2^{-11} = 0,488\%$$
			W tym miejscu również osiągniemy jeszcze niższy wskaźnik fałszywych wskaźań, gdy użyjemy $15$-bitowych znaczników, zamiast $14$-biotwych. Szczegółowa analiza fałszywych pozytywnych wskazań i zapełnienia tablicy znajduje się w pracy \cite{PKCH}.
		\section{Koszt operacji słownikowych}
			Z powyższych rozważań wynika, że złożoność wszystkich operacji słownikowych tj. wstawiania, usuwania i pobierania wartości jest stała lub niemalże stała (patrz: fałszywe pozytywne wskazania). Jest to czas zamortyzowany oraz dotyczy liczby odczytów z pamięci flash. Właściwie po przedstawieniu konstrukcji LogStore powinno się to wydać oczywiste. Dopisywanie nowych par do pliku to pojedyncza operacja - tego wymagają operacje \texttt{insert} i \texttt{remove}. Czasem zamiast wstawiania należy zmodyfikować jakiś wpis, lecz to nie zmienia liczby odczytów z dysku. Obie operacje mogą być obarczone kosztem konieczności ponownego sprawdzenia, podobnie jak operacje \texttt{get\_value}. Wszystkie te operacje działają więc w czasie $O(1+\varepsilon)$ dla konfigurowalnego, małego $\varepsilon$.\\
			\indent Co do zasady kolejne struktury poprawiają jedynie zajętość pamięciową. Jasno widać, że złożoność operacji \texttt{get\_value} nie zmienia się przy przejściu do HashStore. Nieco inaczej jest przy SortedStore, gdzie tworzone jest nowe indeksowanie. Czas ten jest także ograniczony poprzez zastosowanie kubełków zawierających drzewa trie dla konkretnego podziału uniwersum kluczy.\\
			\indent Aby poprawić zajętość pamięci wykonujemy od czasu do czasu kosztowne operacje. Jednak łatwo się przekonać, że dają się one zamortyzować w czasie. Konwersja z LogStore do HashStore jest realizowana w czasie stałym z uwagi na rozmiar obu struktur. Jedyna potencjalnie kosztowna operacja to tworzenie nowego SortedStore'u. Zauważmy jednak, że sortowanie HashStorów działa w czasie liniowym od rozmiaru pliku, czyli jest to stała (bo HashStore ma stały rozmiar). Scalanie również się amortyzuje, gdy dysponujemy wersją wzbogaconą o obliczenia równoległe.
	\chapter{Opis implementacji}
		\section{Struktura programu}
			Program składa się z klas odpowiedzialnych za poszczególne elementy struktury danych. Dla każdej części składowej tj. dla LogStore -- \texttt{Log\_store}, HashStore -- \texttt{Hash\_store} i SortedStore -- \texttt{Sorted\_store} zostały wydzielone osobne klasy. Wyróżniono także klasę opakowującą całą strukturę -- \texttt{Small\_Index\_Large\_Table}.\\
			\indent W programie oprócz podziału na elementy logiczne wydzielono także kilka większych klas -- \texttt{Merge\_heap} (odpowiedzialna za realizację kopca używanego przy scalaniu), \texttt{SILT\_key} (oferująca obliczanie haszu dla klucza oraz wiele przydatnych operatorów do konwersji, porównywania i wyłuskiwania konkretnych bitów z kluczy 160-bitowych używanych w SILT) oraz \texttt{Trie} (klasa modelująca drzewa Trie używane do indeksowania SortedStore).\\
			\indent Program jest napisany w języku C++ (za wyjątkiem generatora losowych danych, który jest napisany w pythonie), bez użycia zaawansowanych struktur ze standardowej biblioteki (STL). Jest kompilowany z flagami \texttt{-std=c++11 -Wall -Wextra -Werror -O2} oraz posiada dwa tryby uruchomieniowe w zależności od zdefiniowania stałej \texttt{DEBUG\_MODE}. Jeśli jest zdefiniowana program wyświetla wiele dodatkowych informacji na ekranie podczas uruchomienia i działania.
	\chapter{Analiza porównawcza}
		\section{Opis eksperymentu}
		\section{Wyniki}
	\chapter{Wnioski}
		\section{Teoretyczne własności}
		\section{Praktyczne osiągnięcia}
	\begin{thebibliography}{3}
		\bibitem{SILT} Hyeontaek Lim, Bin Fan, David G. Andersen, Michael Kaminsky - SILT: A Memory-Efficient, High-Performance Key-Value Store
		\bibitem{PKCH} Bin Fan, David G. Andersen, Michael Kaminsky, Michael D. Mitzenmacher - Cuckoo Filter: Practically Better Than Bloom
		\bibitem{CH} Rasmus Pagh, Flemming Friche Rodler - Cuckoo hashing
		\bibitem{UW} Fragment wykładu ,,Zaawansowane algorytmy i struktury danych'' z Uniwersytetu Warszawskiego \url{http://smurf.mimuw.edu.pl/node/1116} (ostatni dostęp do strony 10.06.2016).
		\bibitem{CHwS} Adam Kirsch, Michael Mitzenmacher, Udi Wieder - More Robust Hashing: Cuckoo Hashing with a Stash
		\bibitem{Cormen} Clifford Rivest, Cormen Thomas H., Leiserson Charles E. - Wprowadzenie do algorytmów
	\end{thebibliography}
\end{document}

