\documentclass[declaration,shortabstract,masc]{iithesis}

\usepackage[utf8]{inputenc}
\newcommand{\cpp}{C\raisebox{0.5ex}{{\tiny\textbf{++}}}}

\polishtitle{Analiza teoretyczna i porównawcza\fmlinebreak efektywnej pamięciowo, wysoce wydajnej\fmlinebreak tablicy asocjacyjnej SILT\fmlinebreak (Small Index Large Table)}
\englishtitle{Theoretical and Comparative Analysis of\fmlinebreak Memory-Efficient, High-Performance\fmlinebreak Associative Array SILT\fmlinebreak (Small Index Large Table)}
\polishabstract{SILT (Small Index Large Table) jest efektywną pamięciowo, wysoce wydajną tablicą asocjacyjną. Stanowi alternatywę dla innych popularnych struktur danych opracowanych na potrzeby rozwiązania problemu przechowywania par klucz-wartość. Została utworzona z myślą o ograniczeniu i zbalansowaniu zużycia pamięci oraz czasu obliczeń. W swojej pracy chcę się skupić na analizie teoretycznej tej struktury danych oraz na porównaniu jej działania z dostępną w bibliotece standardowej języka \cpp\ funkcją \texttt{std::unordered\_map}. W ramach analizy opiszę konstrukcję tej struktury obejmującą trzy główne części składowe -- SortedStore, HashStore oraz LogStore, jak również metodę haszowania kukułczego, która jest w niej wykorzystywana. Zaimplementuję tę strukturę danych i zbadam empirycznie jej wydajność. Porównam ją z wynikami teoretycznymi i doświadczalnymi, przedstawionymi przez jej twórców.}
\englishabstract{\indent SILT (Small Index Large Table) is memory-effective high-performance associative array. It is an alternative for other popular data structures that solve key-value storage problem. It was developed for purpose of reducing and balancing memory usage and computation time. In my thesis I would like to focus on theoretical analysis of this data structure and comparison with \cpp\ standard library's function \texttt{std::unordered\_map}. As part of analysis I will describe its construction consisting of three main parts -- SortedStore, HashStore, LogStore and cuckoo hashing method. I will implement this structure and empirically examine its performance. I~will compare it with theoretical and experimental results, shown by its authors.}
\author{Paweł Guzewicz}
\advisor{dr hab. Marek Piotrów}
\transcriptnum{263664}
\advisorgen{dr. hab. Marka Piotrowa}
%\usepackage{graphicx,listings,amsmath,amssymb,amsthm,amsfonts,tikz}
%\theoremstyle{definition} \newtheorem{definition}{Definition}[chapter]
%\theoremstyle{remark} \newtheorem{remark}[definition]{Observation}
%\theoremstyle{plain} \newtheorem{theorem}[definition]{Theorem}
%\theoremstyle{plain} \newtheorem{lemma}[definition]{Lemma}
%\renewcommand \qedsymbol {\ensuremath{\square}}

\begin{document}
	\chapter{Wprowadzenie}
		\section{Przeznaczenie i zastosowanie}
			Small Index Large Table (SILT) to złożona struktura danych zaprojektowana z~myślą o efektywnej realizacji operacji słownikowych
			\begin{enumerate}
				\item
					Wstawiania: \texttt{insert(Key key, Value value)}
				\item
					Usuwania: \texttt{remove(Key key)}
				\item
					Pobrania wartości: \texttt{get\_value(Key key)}
			\end{enumerate}
			gdzie \texttt{Key} oraz \texttt{Value} oznaczają odpowiednio typ klucza i typ wartości.\\
			\indent W założeniu ma ona szybko obsługiwać powyższe operacje przy zrównoważonym, niedużym zużyciu pamięci. Struktura ta ma być bowiem wykorzystywana w~przypadku konieczności przechowywania dużej ilości danych. Podobnie jak w strukturach bazo-danowych, także tu, kluczowym czynnikiem jest czas dostępu do pamięci dyskowej. (Przyjmujemy dla celów analizy, że jest to pamięć flash, podobnie jak w oryginalnej pracy \cite{SILT}.) Tablica SILT jest zaprojektowana tak, aby oszczędnie korzystać z tejże pamięci. Znajduje swe zastosowanie np. przy przetwarzaniu dużych danych (Big data) jako wariant bazy typu NoSQL.
		\section{Koncepcje i cechy}
			Oczekujemy realizacji wymienionych operacji słownikowych w asymptotycznym czasie stałym. Oczywiście należy brać pod uwagę możliwość wykorzystania cache'u procesora, który zmniejsza czas dostępu do danych i jednocześnie długie czasy dostępu do pamięci zewnętrznej (pamięć flash lub dysk twardy), których chcemy uniknąć.\\
			\indent Uwzględniając możliwości i ograniczenia sprzętowe należy stworzyć strukturę danych dopasowaną możliwie najlepiej do podanych operacji. Nie jest to łatwe, gdy dysponujemy jednym rodzajem magazynu danych. Rozwiązaniem jest połączenie kilku różnych struktur w~jedną. W każdej z tych podstruktur nacisk kładziony jest na inny aspekt -- możliwość szybiego wykonywania operacji modyfikujących strukturę słownika (\texttt{insert(Key key, Value value)} lub \texttt{remove(Key key)}) bądź ograniczenie pamięci kosztem rezygnacji z tych operacji i ograniczenia interfejsu do pobierania wartości (\texttt{get\_value(Key key)}). W~wyniku otrzymujemy kompromisowe rozwiązanie, które okazuje się być bardzo wydajne.\\
			\indent Innym celem, który chcemy osiągnąć jest skalowalność przy ograniczonym zużyciu pamięci przypadającej na jeden klucz.
		\section{Ogólny schemat działania}
			SILT składa się z trzech podstruktur: LogStore, kilku egzemplarzy HashStore i~SortedStore, które są ze sobą powiązane.\\
			\indent Pierwsza z nich odpowiada za szybką obsługę operacji wstawiania i usuwania. Wykorzystuje ona znaną technikę dopisywania kolejnych elementów (par klucz-wartość) na końcu dziennika (logu) -- pliku w pamięci flash. Jest to wysoce wydajne rozwiązanie, lecz nie zapewnia możliwości wyszukiwania, gdyż klucze są umieszczone w pliku w kolejności wstawiania. Do tego celu LogStore w~pamięci RAM przechowuje tablicę z haszowaniem, opartą o metodę haszowania kukułczego, w której zapamiętane są pozycje (przesunięcie od początku pliku) par klucz-wartość w pliku dziennika.\\
			\indent Podstruktury drugiego rodzaju (HashStore'y) są skompresowaną wersją LogStore'ów. Nowa jednostka powstaje w wyniku konwersji przepełnionego LogStore'u. HashStore'y również zawierają tablice z haszowaniem. Pozbawione są jednak informacji o przesunięciu (klucze są sortowane według haszy), przez co efektywniej wykorzystują pamięć. Ich jedynym zadaniem jest obsługa operacji pobrania wartości, wobec czego mogą być strukturami tylko do odczytu. Podobnie jak w LogStore'rze wszystkie dane trzymane są w pamięci flash, a w pamięci RAM zostaje umieszczony filtr (okrojona tablica z haszowaniem), który wstępnie określa, czy dany klucz może znajdować się w tym magazynie (przy niskim odsetku fałszywych pozytywnych wskazań, z ang. \textit{false positives}).\\
			\indent W trzeciej podstrukturze (SortedStore) w pamięci flash trzymane są posortowane dane. Eliminuje to konieczność używania tablicy z haszowaniem do wyszukiwania pozycji w pliku. Przy takim podejściu potrzebowalibyśmy jednak czasu $O\left(\log n\right)$, używając wyszukiwania binarnego, na znalezienie naszej wartości. Również i tu możemy zastosować efektywniejsze rozwiązanie -- indeksowanie za pomocą drzew trie trzymanych w pamięci RAM.
	\chapter{Struktura danych Small~Index~Large~	Table}
		\section{LogStore}
			Pary klucz-wartość przechowywane w strukturze SILT mają pewnien stały, ustalony rozmiar. Może on jednak być dosyć duży, więc zostanie zapisany w pamięci flash. W pamięci RAM przechowujemy zaś tablicę z haszowaniem kukułczym (a dokładniej w pewnym jego wariancie -- haszowaniu kukułczym dla częściowych kluczy).\\
			\indent W podstawowej wersji haszowania kukułczego używamy tablicy kubełków (ustalonego rozmiaru) oraz dwóch funkcji haszujących $h_1$ i $h_2$. Dla danego klucza wyznaczają one dwie potencjalne lokalizacje (kubełki), w które można wstawić nowy klucz. Jeśli w którymś z kubełków jest wolne miejsce, to wstawiamy do tego niezapełnionego kubełka nasz klucz. W przeciwnym razie wstawiamy nowy klucz do jednego z kubełków mimo zapełnienia i wypychamy znajdujący się w nim klucz (można myśleć, że podrzucamy kukułcze jajo do gniazda). Następnie wypchnięty klucz przenosimy do jego alternatywnej lokalizacji (drugiego kubełka). To również może powodować konieczność wypchnięcia kolejnego klucza. Kontynuujemy proces wstawiania i wypychania aż do znalezienia wolnego kubełka lub do momentu, gdy przekroczymy maksymalną liczbę wypchnięć. Jeśli osiągniemy limit iteracji i nie udało nam się wstawić elementu, oznacza to, że tablica jest już prawie pełna. Wówczas należy ponownie shaszować elementy tablicy używając nowych funkcji haszujących. Może być też konieczne zwiększenie rozmiaru tablicy (używając techniki podwajania wielkości utrzymujemy zamortyzowany czas stały).\\
			\indent W naszym przypadku, z uwagi na potencjalnie duży rozmiar klucza, unikamy wykorzystywania go w całości. SILT używa 160-bitowych kluczy, które pochodzą z rozkładu jednostajnego i są wstępnie obliczane przy użyciu algorytmu SHA-1. Dzięki temu otrzymujemy stałą długość klucza. Aby jeszcze zmniejszyć rozmiar kluczy w tablicy z haszowaniem kukułczym stosujemy tzw. częściowe klucze lub odciski palca (z ang. \textit{fingerprints}).\\
			\indent Haszowanie kukułcze dla częściowych kluczy (z ang. \textit{partial-key cuckoo hashing}) \cite{PKCH} to zmodyfikowana wersja haszowania kukułczego. Zamiast zapamiętywać cały 160-bitowy klucz zapamiętujemy pewnien jego znacznik. To w oczywisty sposób uniemożliwia odtworzenie z niego klucza i tym samym powoduje problem w momencie konieczności obliczenia alternatywnego kubełka. Wystarczy więc wraz z danymi przechowywanymi w komórce tablicy związać informację o drugim kubełku. W LogStore'rze będziemy używali do tego celu $14$-bitowych znaczników, zatem kubełki w tablicy będą indeksowane liczbami od $0$ do $2^{14} - 1$. Łączny rozmiar pojedynczego wpisu w kubełku to 4 bajty. Zawiera on 14 bitów znacznika, jeden bit określający rodzaj operacji ($0$ dla usuwania, $1$ dla wstawiania), jeden bit zajętości ($0$, gdy wpis jest nieużywany (wolny) oraz $1$, gdy zajęty) oraz dwubajtową wartość przesunięcia od początku pliku (z ang. \textit{offset}).\\
			\indent Znaczniki odpowiadają funkcjom $h_1$ oraz $h_2$ zaaplikowanym do klucza. Aby sprawdzić czy dany wpis w tablicy dotyczy klucza, dla którego właśnie szukamy wartość, należy obliczyć dla niego znaczniki i sprawdzić czy w kubełku indeksowanym $h_1$ znajduje się znacznik $h_2$ i odwrotnie. Schematycznie (pomijając informacje zakodowane w dwóch dodatkowych bitach oraz głębokość kubełków) dane są związane relacją \texttt{hash\_table[h1] = (h2, offset)}. Autorzy pracy \cite{SILT} ustalili eksperymentalnie optymalną głębokość kubełków na $4$. Co daje łącznie $2^{16}$ wpisów w tablicy. Znaczniki obliczane są jako rozłączne fragmenty $14$-bitowe wybrane spośród najmłodszych bitów 160-bitowego klucza. Przyjmujemy również ograniczenie na liczbę iteracji wstawień i wypychań na $64$.\\
			\indent Warto zaznaczyć, że w oryginalnej wersji znacznik zawiera $15$ bitów. Uniemożliwia to jednak kodowanie operacji. Dla celów analizy wydajności LogStore'u pominięcie operacji usuwania nie ma żadnego znaczenia. Operacje wstawiania i usuwania elementów z LogStore'u z punktu widzenia struktury są niemal identyczne. Różnią się jedynie zapisaniem informacji o rodzaju operacji, wobec czego analiza usuwania sprowadza się do analizy wstawiania. Taki wybór rozmiaru znacznika niesie za sobą jednak dwie konsekwencje. Z jednej strony zyskujemy dwukrotnie większą przestrzeń adresową, przez co mamy do dyspozycji $2^{17}$ wpisów w tablicy. Jednak pociąga to za sobą konieczność rozszerzenia rozmiaru wpisu o $2$ bajty. Zwiększa się wówczas rozmiar offsetu, gdyż zmienna $16$-bitowa byłaby niewystarczająca. Wydaje się, że autorzy pracy postąpili nieco wbrew przyjętej przez nich konwencji. W pozostałych strukturach zawsze starali się zaoszczędzić pamięć, jeśli to tylko było możliwe. Ponadto ograniczyli także odrobinę funkcjonalność struktury danych. Zmianę tę należy potraktować jako kosmetyczną poprawkę.\\
		\section{HashStore}
			HashStore to kompaktowa wersja LogStore. W momencie, gdy osiągniemy zapełnienie LogStore jest konwertowany do HashStore. Konwersja polega na przepisaniu danych z dziennika (logu) do nowego pliku tak, aby były uporządkowane w kolejności haszy, a nie -- jak poprzednio -- w kolejności wstawienia. Dzięki temu możemy zmniejszyć liczbę koniecznych do zapamiętania informacji. Wystarczy we wpisach tablicy pamiętać jedynie znaczniki, pomijając przesunięcia. Można je bowiem wyliczyć, korzystając z ich uporządkowania i ze stałego rozmiaru par klucz-wartość.\\
			\indent Po zgromadzeniu dostatecznie dużej liczby (ustalona z góry stała - w naszym przypadku równa $15$) HashStore'ów należy je posortować i scalić z dotychczasowym SortedStore'em. Proces sortowania przebiega sekwencyjnie. Sortujemy po kolei każdy HashStore z osobna. Do tego celu używamy algorytmu sortowania pozycyjnego (\texttt{radix\_sort}) dla kluczy $160$-bitowych. Składa się on z $10$ przebiegów algorytmu sortowania przez zliczanie (\texttt{counting\_sort}) począwszy od najmniej znaczących par bajtów (sortujemy przez zliczanie $16$-bitowe fragmenty). Liczba kubełków jest stała i nieduża, równa $2^{16} = 65536$.\\
			\indent Kolejnym krokiem, mając posortowane HashStore'y, jest scalanie. Będziemy dokonywać wielotorowego scalania (z ang. \textit{multiway merge}). Będziemy wybierać element, który ma się znaleźć w nowym SortedStore'rze na podstawie jego klucza, rodzaju operacji oraz chronologii. Jeśli mamy tylko jeden klucz, to go wybieramy. W przypadku konfliktu (2 lub więcej takich samych) sprawdzamy, który z nich był najpóźniej dodany (czyli posiada najświeższą informację). Następnie, po wyborze klucza, odczytujemy rodzaj operacji (dla kluczy z SortedStore jest to zawsze dodawanie) i dopisujemy go bądź nie do pliku wynikowego. Do wybierania kluczy używamy algorytmu znanego z sortowania przez scalanie (tam scalając dwie listy przesuwamy się wskaźnikiem na kolejne elementy, tu mamy do czynienia z większą liczbą list i wskaźników). Problematyczne jest wybranie kandydata spośród $15$ lub $16$ kluczy (w zależności, czy był już wcześniej jakiś SortedStore). Aby przyspieszyć i ułatwić ten proces używamy kopca binarnego do przechowywania kluczy na pozycjach wskazywanych przez wskaźnik. W danej chwili w kopcu znajduje się $15$ (lub $16$) kluczy. Wybierając element ze szczytu kopca (uporządkowanego według naszego kryterium) decydujemy czy dopisać go do pliku czy nie, a następnie usuwamy z kopca wszystkie elementy o tym samym kluczu oraz przywracamy własność kopca, a dla elementów, które usunęliśmy przesuwamy wskaźnik na ich liście na kolejny klucz i wstawiamy go do kopca. Iterujemy tę operację, aż kopiec będzie pusty.
		\section{SortedStore}
			Po scaleniu HashStore'ów budujemy indeksowanie dla posortowanego pliku złożonego z par klucz-wartość. Będzie ono oparte na drzewach trie (czyt. \textit{traj}). Są to drzewa binarne o krawędziach etykietowanych zerami i jedynkami (można pominąć etykiety w pamięci -- $0$ odpowiada zawsze lewemu poddrzewu, a $1$ prawemu). Dla danej tablicy liczb w zapisie binarnym każdy liść w takim drzewie odpowiada dokładnie jednej z nich. Numer liścia (liczony od lewej) wyznacza indeks w tablicy. W naszym przypadku ten indeks to przesunięcie pary klucz-wartość w pliku wynikowym. Każdy wierzchołek wewnętrzny odpowiada najdłuższemu wspólnemu zero-jedynkowemu prefiksowi. Ponadto w drzewie tym trzymane jest jedynie tak wiele wierzchołków na ścieżce od korzenia do liścia, żeby etykiety krawędzi tworzyły najkrótszy jednoznaczny prefiks (a ang. \textit{shortest unique prefix}). Aby zapamiętać drzewo trie, zamiast kosztownych pamięciowo wskaźników, zapamiętujemy w każdym wierzchołku liczbę wierzchołków w jego lewym poddrzewie. W ten sposób jesteśmy w stanie łatwo obliczyć numer każdego liścia. Taka reprezentacja jest też łatwa w kodowaniu. Wystarczy zapisać ją zgodnie z rekurencyjną zależnością
			$$representation\left(T\right) = |L| \cdot representation\left(L\right) \cdot representation\left(R\right),$$
			gdzie T to całe drzewo, L - lewe poddrzwo, R - prawe poddrzewo, |L| to liczba wierzchołków w lewym poddrzewie, a $\cdot$ to operacja sklejenia.\\
			\indent Aby odnaleźć liść w drzewie posługujemy się rekurencyjnym algorytmem, który odwiedza odpowiednie poddrzewo i zapamiętuje sumę etykiet wierzchołków, w których wybrał przejście do prawego poddrzewa (tym samym liczbę liści, które zostały na lewo od poszukiwanego wierzchołka).\\
			\indent Aby ograniczyć potencjalnie kosztowny proces rekurencyjnego przeszukiwania można podzielić drzewa trie do $2^k$ kubełków na podstawie $k$ pierwszych bitów klucza. Pozwala to na dobranie odpowiedniego $k$ np. równego 10, aby zredukować oczekiwaną liczbę kluczy, które znajdą się w poszczególnych drzewach trie. Autorzy podają, że w przypadku danych rozmiaru $2^16$ oczekujemy $2^{16-19} = 2^6$ kluczy w drzewie, a z dużym prawdopodobieństwem nie więcej niż $2^8$, czyli stałą liczbę.\\
			\indent Autorzy pracy o SILT przedstawili jeszcze jedną, skomplikowaną reprezentację drzew trie, która jest w stanie jeszcze efektywniej zapamiętywać informacje o kluczach. Średnio wystarcza w niej około $0,4$ bajta na sekundkę.
		\section{Przepływ danych wewnątrz struktury}
			Jak widać z opisanej konstrukcji przepływ danych w strukturze jest spowolniony, przypomina przepychanie informacji (od LogStore'u do SortedStore'u poprzez HashStore'y). Leniwe wstawianie i usuwanie oraz konwersje ze struktur jednego typu do innego występujące co jakiś czas mają na celu zamortyzowanie czasu operacji.
		\section{Rozszerzenia}
			Struktura SILT może być rozszerzona tak, aby umożliwić równoległe przetwarzanie transakcji (w rozumieniu bazy danych). Dodatkowo można jeszcze bardziej amortyzować czas, który jest poświęcany na konwersję z jednego magazynu danych do drugiego stosując konwersję równolegle do obsługi zapytań. Taka wersja została zaimplementowana i opisana przez twórców SILT. Jedną z najistotniejszych różnic jest sposób scalania HashStore'ów, który jest realizowany w tle. Jest on jednak rozciągnięty w czasie, a kolejne struktury scalane są jedna po drugiej, więc struktura kopca, używana do scalania nie jest używana.\\
			\indent Innym ważnym aspektem jest możliwość odtwarzania danych po awarii. Do tego celu wystarczy zapamiętywać w osobnym dzienniku w pamięci flash kolejne operacje zlecone do LogStore'u. Po awarii, aby odzyskać tablicę trzymaną w pamięci RAM, należy po kolei odtworzyć operacje zapisane w dzienniku. Dla pozostałych magazynów możemy pamiętać kopię informacji z pamięci RAM na dysku, gdyż są one strukturami tylko do odczytu.\\
			\indent W przypadku dużego obciążenia pamięci RAM można, kosztem czasu dostępu, usunąć z niej tablice indeksujące dla HashStore'ów i SortedStore'ów, pod warunkiem, że zostaną zapamiętane w pamięci flash. Dla SortedStore możliwe jest nawet kompletne pominięcie struktury drzew trie, ale wiąże się to z kosztownym przeszukiwaniem binarnym.\\
			\indent Jeśli chcielibyśmy wykorzystać strukturę SILT do przechowywania par klucz-wartość o zmiennej długości, to możemy użyć techniki przechowywania par (przesunięcie, fragment pary klucz-wartość), gdzie przesunięcie odnosi się do odrębnego pliku, w którym zapamiętane są właściwe dane.\\
			\indent W obrębie jednej bazy danych możemy korzystać z kilku magazynów SILT. Każdy odpowiedzialny jest wówczas za osobny (rozłączny z innymi) podzbiór kluczy.
	\chapter{Analiza teoretyczna}
		\section{Haszowanie kukułcze}
			Haszowanie kukułcze szeroko opisane w pracy \cite{CH} ma oszacowanie czasu operacji na poziomie $O(1)$. Wynika to z analizy prawdopodobieństwa wystąpienia łańcucha kolizji podczas wstawiania. Analizowany jest graf dwudzielny wyznaczony przez kolejne kubełki. Kluczowe w analizie jest ograniczenie możliwej liczby iteracji oraz wyznaczenie optymalnej stałej. Po osiągnięciu takiej liczby operacji następuje konieczność przehaszowania tablicy.\\
			\indent Różnicą w analizie, w kontekście haszowania za pomocą częściowych kluczy, jest inny sposób zapewnienia stałego czasu wstawiania. Oczywiście limit iteracji jest zachowany, natomiast tablica nie jest już ponownie haszowana. Decydujemy się wtedy na wycofanie transakcji (wstawiania lub usuwania). Takie odtwarzanie trwa nie więcej niż trwało jego nieudane wstawianie do tablicy z haszowaniem. Wobec czego ta operacja ma co najwyżej czas dwukrotnie dłuższy, czyli dalej stały.\\
			\indent Podobna uwaga dotyczy obliczania wartości funkcji haszujących. W zwykłej wersji obliczaliśmy ją gdy była potrzebna (dla danego klucza mogła być więc obliczana wielokrotnie), a w zmodyfikowanej wersji wszystkie alternatywne kubełki są od razu obliczone. Należy podkreślić, że obliczenie to realizowane jest kosztem dwóch instrukcji maszynowych (wyłuskujemy odpowiedni fragment klucza). Ten klucz został wcześniej shaszowany funkcją SHA-1, ale to było zrobione tylko raz, więc jest to czas stały. W pewnym sensie odrobinę zyskujemy czasowo na obliczaniu funkcji haszujących, tracimy zaś na pamięci.\\
			\indent Poprzednia uwaga nie jest kluczowa w kontekście złożoności. Na potrzeby uzasadnienia poprawności oszacowań dla czasu działania haszowania wystarczy zauważyć, że założenia dotyczące oryginalnego modelu nie zostały osłabione. Funkcje haszujące są dobrze dobrane i niezależne (fragmenty klucza, z których wybieramy znaczniki nie pokrywają się), co wynika z własności kluczy generowanych algorytmem SHA-1.\\
			\indent Ostatnią rzeczą, która wymaga komentarza jest użycie kubełków o rozmiarze większym niż $1$. Wpływa to pozytywnie na możliwość wypełnienia tablicy. Dzięki temu w \cite{SILT} osiągnięto zajętość na poziomie $93$\%. Inny podobny pomysł, tym razem bazujący na dodaniu małego schowka, który ma poprawić zajętość tablicy jest opisany w pracy \cite{CHwS}. Oczywiście aby zachować własności probabilistyczne podczas wypychania kluczy z alternatywnego kubełka losujemy, który wpis ma być usunięty (w naszym przypadku losujemy liczbę od $0$ do $3$). Możemy to zrobić, gdyż kolejność wpisów wewnątrz kubełka nie ma znaczenia.
		\section{Fałszywe pozytywne wskazania}
			Dla każdego klucza mamy do dyspozycji 2 kubełki po 4 wpisy, w sumie $8=2^3$. Zatem prawdopodobieństwo fałszywie pozytywnego wskazania wynosi $\frac{2^3}{2^14} = @^{-11} = 0,488$\%. W tym miejscu również osiągniemy jeszcze niższy wskaźnik fałszywych wskaźań, gdy użyjemy $15$-bitowych znaczników, zamiast $14$-biotwych. Szczegółowa analiza fałszywych pozytywnych wskazań i zapełnienia tablicy znajduje się w pracy \cite{PKCH}.
		\section{Koszt operacji słownikowych}
			Z powyższych rozważań wynika, że złożoność wszystkich operacji słownikowych tj. wstawiania, usuwania i pobierania wartości jest stała lub niemalże stała (patrz: fałszywe pozytywne wskazania). Jest to czas zamortyzowany oraz dotyczy liczby odczytów z pamięci flash. Właściwie po przedstawieniu konstrukcji LogStore powinno się to wydać oczywiste. Dopisywanie nowych par do pliku to pojedyncza operacja - tego wymagają operacje \texttt{insert} i \texttt{remove}. Czasem zamiast wstawiania należy zmodyfikować jakiś wpis, lecz to nie zmienia liczby odczytów z dysku. Obie operacje mogą być obarczone kosztem konieczności ponownego sprawdzenia, podobnie jak operacje \texttt{get\_value}. Wszystkie te operacje działają więc w czasie $O(1+\varepsilon)$ dla konfigurowalnego, małego $\varepsilon$.\\
			\indent Co do zasady kolejne struktury poprawiają jedynie zajętość pamięciową. Jasno widać, że złożoność operacji \texttt{get\_value} nie zmienia się przy przejściu do HashStore. Nieco inaczej jest przy SortedStore, gdzie tworzone jest nowe indeksowanie. Czas ten jest także ograniczony poprzez zastosowanie kubełków zawierających drzewa trie dla konkretnego podziału uniwersum kluczy.\\
			\indent Aby poprawić zajętość pamięci wykonujemy od czasu do czasu kosztowe operacje. Jednak łatwo się przekonać, że dają się one zamortyzować w czasie. Konwersja z LogStore do HashStore jest realizowana w czasie stałym z uwagi na rozmiar obu struktur. Jedyna potencjalnie kosztowna operacja to tworzenie nowego SortedStore'u. Zauważmy jednak, że sortowanie HashStorów działa w czasie liniowym od rozmiaru pliku, czyli jest to stała (bo HashStore ma stały rozmiar). Scalanie również się amortyzuje, gdy dysponujemy wersją wzbogaconą o obliczenia równoległe.
	\chapter{Analiza porównawcza}
		\section{Opis eksperymentu}
		\section{Wyniki}
	\chapter{Wnioski}
		\section{Teoretyczne własności}
		\section{Praktyczne osiągnięcia}
	\begin{thebibliography}{3}
		\bibitem{SILT} Hyeontaek Lim, Bin Fan, David G. Andersen, Michael Kaminsky - SILT: A Memory-Efficient, High-Performance Key-Value Store
		\bibitem{PKCH} Bin Fan, David G. Andersen, Michael Kaminsky, Michael D. Mitzenmacher - Cuckoo Filter: Practically Better Than Bloom
		\bibitem{CH} Rasmus Pagh, Flemming Friche Rodler - Cuckoo hashing
		\bibitem{CHwS} Adam Kirsch, Michael Mitzenmacher, Udi Wieder - More Robust Hashing: Cuckoo Hashing with a Stash
	\end{thebibliography}
\end{document}
